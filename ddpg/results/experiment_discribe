实验描述：
实验01：在上一个版本的基础上将网络规模扩大到原始的3倍(times=3) 实验结果：成功,效果更好
实验02,在实验01(cu01:af74112)的基础上加入论文使用的权值初始化方法 实验结果:失败, 效果变差收敛变慢
实验03：在实验01的基础上去掉奖励值的归一化 实验结果：成功,相当于放大loss或者是增大学习率，收敛更快性能可以接受
实验04：在实验03的基础使用swish替换relu 实验结果：失败,效果不如relu
实验05：在实验03的基础上加入探索噪音收敛 实验结果:失败,误差抖动还是很厉害,不如原有的策略,按照输出来控制噪音大小
实验06：在实验03的基础上使用奖励归一化缩小最大值为4k 实验结果：没有明显变化
实验07：改变控制AGV的模型参数 实验结果：可以收敛,能够实现相近的控制效果,但收敛速度较慢会出现震荡
实验08：改变控制AGV的模型参数, 改变控制力矩最大范围 实验结果：不收敛或收敛太慢
实验09：改变误差计算方式,扩大error bound 到2  实验结果：可以收敛,loss下降曲线感觉学习率太高了
实验10: 改变误差计算方式, error bound 到 2  实验结果：可以收敛
实验11: 按照梯度更新rewardFunction 使得reward限制到-1~0之间， error boud 到1
实验结果：按照速度奖励与误差奖励的梯度大小关系，速度小于0.3的时候会优先优化速度，当速度大于0.3时会优先优化误差，在误差小于0.1时又会去优化速度
代码更改过，有可能不是实验的代码；
实验12：噪音收敛后停止模型参数的更新， 但不停止target网络的soft更新  实验结果：可以保持收敛的效果；
实验13：将历史状态长度设为2 对比 实验14 长度为4 的对照实验   实验结果：状态历史长度越长收敛越快；
但长度为2的实验最终收敛的性能更好；
实验14：与实验13对比，测试最终收敛性能 实验结果： 最终结果速度在1.4左右，误差在0.5左右
实验15: 在DDPG的网络结构的基础上去掉BN层,batch_size:128,Actor_lr:1.0E-4,Critic_lr:1.0E-3  实验结果:模型不收敛 震荡
实验16: 在实验15的基础上改变参数batch_size:64,Actor_lr:1.0E-5,Critic_lr:1.0E-4  实验结果:较实验15要稳定,但是表现还是很糟糕
实验17: 在实验16的参数上恢复BN层  实验结果:表现不好,震荡
实验18: 在实验17的基础上恢复原来的学习率(都增大) 实验结果:可以收敛
实验19: 加入"a deeper look of experience replay"d combine-DQN 算法用当前的Sara更新一次再用batch更新一次 结果:很糟糕震荡不收敛.
实验20: 增加maxtime次数由300到600 实验结果:有效,后期误差在零的上下波动
实验21: 调整replay_buffer size 由1.0E6到1.0E4 实验结果:1.0E4大小的buffer比2.0E4大小的buffer收敛更快.但是AGV运行到后期会失去稳态
*实验22: 继续减小buffer size 到5.0E3 实验结果:不稳定,收敛变慢,模型最终表现不好
*实验23: 增大maxtime由600到1000 实验结果:最终模型在一千步以内可以保证稳定,超出一段时间就开始震荡,说明超出一千步以后的状态之前没遇到过,考虑采用随机初始化